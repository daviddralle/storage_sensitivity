{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter (Python) notebook to generate the study sites shapefile, and to extract precipitation, evapotranspiration, and EVI data from geospatial datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import glob\n",
    "import geopandas as gp\n",
    "import georasters as gr\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sitelist = [str(item) for item in [11154700,\n",
    "11200800,\n",
    "11299600,\n",
    "11046360,\n",
    "11180825,\n",
    "11046300,\n",
    "11180960,\n",
    "11182500,\n",
    "11449500,\n",
    "11379500,\n",
    "11284400,\n",
    "11224500,\n",
    "11253310,\n",
    "11141280,\n",
    "11151300,\n",
    "11469000,\n",
    "11111500,\n",
    "11176400,\n",
    "11172945,\n",
    "11132500,\n",
    "11475800,\n",
    "11134800,\n",
    "11180900,\n",
    "11475560,\n",
    "11476600]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataframe of site properties and polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins_df = gp.read_file('../data/basins/basins18_utm.shp')[['SITE_NO', 'geometry']]\n",
    "usgs_df = gp.read_file('../data/USGS_gages/USGS_Streamgages-NHD_Locations.shp')[['SITE_NO', 'STATION_NM']]\n",
    "sites = basins_df.merge(usgs_df, on='SITE_NO').set_index('SITE_NO').loc[sitelist]\n",
    "sites['gauge_id'] = sites.index\n",
    "\n",
    "# Add Dry Creek catchment, which is not USGS\n",
    "dry = gp.read_file('../data/dry_creek_polygon/dry.shp')\n",
    "sites = sites.append({'gauge_id':'00000000', 'geometry':dry.geometry.values[0], 'STATION_NM':'Dry Creek'}, ignore_index=True)\n",
    "sites.index = sites.gauge_id\n",
    "sites.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Save sites to shapefile\n",
    "sites.to_file('../data/sites.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download USGS flow data (fixed dates 1980 - 2017)\n",
    "# do not download new data if flow data in ../data/flowdata/\n",
    "def getFlow(site):\n",
    "    try: \n",
    "        df = pd.read_csv('../data/flow_data/' + site + '.csv', parse_dates=True, index_col='datetime')\n",
    "    except:\n",
    "        url = 'https://waterdata.usgs.gov/nwis/dv?cb_00060=on&format=rdb&site_no=' + site + '&referred_module=sw&period=&begin_date=2018-01-01&end_date=2018-10-1'\n",
    "        df = pd.read_csv(url, header=31, delim_whitespace=True)\n",
    "        df.columns = ['usgs', 'site', 'datetime', 'q', 'a']\n",
    "        df.index = pd.to_datetime(df.datetime)\n",
    "        df = df[['q']]\n",
    "        df.q = df.q.astype(float, errors='ignore')\n",
    "        df.to_csv('../data/flow_data/' + site + '.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and save discharge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a collection of dataframes with flow\n",
    "discharge_dfs = []\n",
    "dates = pd.date_range('1980-01-01', '2018-12-31')\n",
    "for i,row in sites.iterrows():\n",
    "    gagestr = row.gauge_id\n",
    "    if gagestr=='00000000':\n",
    "        continue\n",
    "    try:\n",
    "        df = getFlow(gagestr)\n",
    "    except:\n",
    "        sites = sites.loc[sites.gauge_id!=gagestr]\n",
    "        continue\n",
    "        \n",
    "    rng = df.index\n",
    "    df = pd.DataFrame.from_dict({gagestr:df.q.values}).set_index(rng)\n",
    "    df = df*2.44657555e12 # convert to mm^3/day\n",
    "    area = row.geometry.area*1e6 # area to mm\n",
    "    df = df/area # flow in mm/day\n",
    "    df = df.reindex(dates)\n",
    "    discharge_dfs.append(df)\n",
    "\n",
    "# Incorporate dry creek discharge data\n",
    "dryq = pd.read_csv('../data/dry_creek_discharge.csv', index_col=0, parse_dates=True)\n",
    "discharge_dfs.append(dryq)\n",
    "discharge_df = pd.concat(discharge_dfs, axis=1)\n",
    "discharge_df = discharge_df.resample('M',label='left', loffset='1D', closed='right', how=lambda x: x.values.sum())\n",
    "discharge_df.to_csv('../data/discharge_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract raster data from monthly PRISM rainfall and ET rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of precip raster files\n",
    "precip_files = glob.glob('../data/monthly_ppt/2*/*.tif')\n",
    "data = np.zeros((len(precip_files), len(sites)))\n",
    "cols = sites.gauge_id.values\n",
    "dts = []\n",
    "for i,f in enumerate(precip_files):\n",
    "    rast = gr.from_file(f)\n",
    "    rast.nodata_value = np.nan\n",
    "    # get timestamp\n",
    "    dts.append(pd.to_datetime(f[-10:-4], format='%Y%m'))\n",
    "    # get this month's precip for each site\n",
    "    datacurr = [item.raster.data for item in rast.clip(sites.geometry)]\n",
    "    data[i,:] = [np.mean(item[item>=0]) for item in datacurr]\n",
    "\n",
    "precip = pd.DataFrame(data, index=dts, columns=cols).sort_index()\n",
    "\n",
    "# Get list of ET raster files\n",
    "et_files = glob.glob('../data/monthly_ET/*.tif')\n",
    "data = np.zeros((len(et_files), len(sites)))\n",
    "cols = sites.gauge_id.values\n",
    "dts = []\n",
    "for i,f in enumerate(et_files):\n",
    "    rast = gr.from_file(f)\n",
    "    rast.nodata_value = np.nan\n",
    "    dts.append(pd.to_datetime(f[-11:-4], format='%m-%Y'))\n",
    "    datacurr = [item.raster.data for item in rast.clip(sites.geometry)]\n",
    "    data[i,:] = [np.mean(item[item>=0]) for item in datacurr]\n",
    "et = pd.DataFrame(data, index=dts, columns=cols).sort_index()\n",
    "\n",
    "# Save extracted data\n",
    "precip.to_csv('../data/precip_sites.csv')\n",
    "et.to_csv('../data/et_sites.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and save EVI extracted from Earth Engine \n",
    "\n",
    "The Google Earth Engine script to extract and download the EVI geojson data can be found [HERE](https://code.earthengine.google.com/a15f4246f8bbc070913a9c0217a1b031). The file generated by Earth Engine is located at `../data/mean_modis_evi.geojson`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evi = gp.read_file('../data/modis_mean_evi.geojson')\n",
    "evi['datetime'] = pd.to_datetime(evi.date, format='%Y_%m_%d')\n",
    "\n",
    "# the EarthEngine multplier for EVI product is 0.0001\n",
    "evi = 0.0001*evi.pivot(index='datetime', columns='gauge_id', values='mean').sort_index().resample('MS').mean()\n",
    "evi = evi[sites.index]\n",
    "evi.to_csv('../data/evi_sites.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3k",
   "language": "python",
   "name": "py3k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
